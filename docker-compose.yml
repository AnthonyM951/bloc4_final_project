version: "3.9"

services:
  web:
    build: .
    container_name: videoapp_web
    env_file: .env
    ports:
      - "5000:5000"
    depends_on:
      redis:
        condition: service_healthy
      ollama:
        condition: service_started
    restart: unless-stopped
    environment:
      - OLLAMA_URL=http://ollama:11434

  worker:
    build: .
    container_name: videoapp_worker
    env_file: .env
    command: >
      python -m celery
      -A worker:celery
      worker
      -l info
    depends_on:
      redis:
        condition: service_healthy
      ollama:
        condition: service_started
    restart: unless-stopped
    user: "1000:1000"
    environment:
      - OLLAMA_URL=http://ollama:11434

  redis:
    image: redis:7-alpine
    container_name: videoapp_redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 20
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: videoapp_ollama
    ports:
      - "11434:11434" # Expose to host as well
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped

volumes:
  ollama_data:
